# StepEの指示内容（実際の学習＆評価）

これから StepE（実際の学習＆評価）を進めたいです。  
基本方針・設計は以下に書かれている内容に従います。

- `docs/今後の方針.md`
- `LLM_ファインチューニング/Step2の指示内容.md`
- `LLM_ファインチューニング/Step3_まとめ.md`
- `LLM_ファインチューニング/B_ベースライン結果_qwen3_rag_top3_direct.md`
- `LLM_ファインチューニング/B_ベースライン結果_qwen3_rag_top3_cot.md`
- `LLM_ファインチューニング/StepC_まとめ.md`
- `LLM_ファインチューニング/StepD_実験計画.md`
- `LLM_ファインチューニング/StepD_タスク2_設計.md`
- `LLM_ファインチューニング/StepD_学習コマンド案.md`
- `LLM_ファインチューニング/StepD_学習実験ログテンプレート.md`
- 実装済み:
  - `app/core/prompts.py`
  - `scripts/build_finetune_dataset.py`
  - `scripts/evaluate_multiple_choice.py`
  - `scripts/train_qwen_law_ft.py`
  - 4つの JSONL:
    - `results/finetune/ft_direct_full_norag.jsonl`
    - `results/finetune/ft_cot_full_norag.jsonl`
    - `results/finetune/ft_direct_full_rag_top3.jsonl`
    - `results/finetune/ft_cot_full_rag_top3.jsonl`

---

## 共通ルール

- 作業は **既存の作業ブランチ** か、新しいブランチ（例：`feature/llm-ft-train-run`）で行ってください。  
  `main` を直接変更しないでください。
- GPU を用いた重い学習ジョブの **実行可否は、ユーザー（人間）が決める前提**としてください。  
  コマンド案は提示して構いませんが、「実行した」と書かないでください。
- `train_qwen_law_ft.py` のコード変更はこのステップで行っても構いませんが、  
  変更内容は必ず Markdown に記録してください。
- 各タスクの最後に、  
  `LLM_ファインチューニング/StepE_タスクX_結果.md`  
  のような Markdown に「何を変更／提案したか」「実行は人間に委ねる」旨をまとめてください。

---

## タスクE-1：train_qwen_law_ft.py を「本番モード」にする設計・実装

### 目的

現在は `trainer.train()` がコメントアウトされており、  
コマンドを叩いても学習が走りません。  
これを、安全に ON/OFF できるようにします。

### やってほしいこと

1. `scripts/train_qwen_law_ft.py` を確認し、  
   現在 `trainer.train()` / `save_model()` / `tokenizer.save_pretrained()` がコメントアウトされていることを前提にしてください。

2. 以下の2パターンのうち、どちらかの方針で実装案を出し、その後コードを変更してください。

   - パターン1：`--do-train` フラグ方式
     - 例：`--do-train` が指定されたときだけ `trainer.train()` と保存を実行する。
     - 指定されない場合は、これまで通り「セットアップだけして終了」。
   - パターン2：`--max-steps` / `--subset-ratio` 方式
     - すでに `TrainingArguments` にある `max_steps` を活用しつつ、
       - 「デフォルトは小さめのstep数でテスト」
       - 明示的に `--max-steps -1` などを指定した場合のみフル学習
     - などの安全策を設計。

3. 上記方針に基づき、`train_qwen_law_ft.py` に以下の変更を加えてください。

   - `argparse` に新しい引数（`--do-train` や `--max-steps` 上書きなど）を追加。
   - `main()` の最後で、条件に応じて
     - `trainer.train()`
     - `trainer.save_model(...)`
     - `tokenizer.save_pretrained(...)`
     を呼ぶようにする。
   - 「このスクリプトをそのまま実行すると何が起きるか」を docstring やコメントで明示。

4. 変更内容と、今後の推奨利用方法（例：「最初は `--do-train` 付き & `--num-epochs 1` でテスト」）を  
   `LLM_ファインチューニング/StepE_タスク1_結果.md` にまとめてください。

> ※ このタスクでは **実際に学習コマンドを実行する必要はありません**。  
>  「実行可能な状態のスクリプト」にするところまでです。

---

## タスクE-2：小規模トライアル学習の計画（＆必要なら疑似実行）

### 目的

いきなり 140問フル×3エポック学習を走らせるのはリスキーなので、  
**ごく小さな subset でのテスト学習計画**を立てます。

### やってほしいこと

1. `StepD_学習コマンド案.md` の候補1・候補2（パターンA/B）をベースに、  
   「トライアル用に縮小したコマンド案」を作成してください。

   例：

   - データを 10〜20レコードに絞る（必要なら `train_qwen_law_ft.py` 側で `--max-samples` を追加する設計も可）
   - `--num-epochs 1`
   - `--max-steps` を小さい値にする（例：`--max-steps 50`）

2. それぞれについて、

   - 何を確認したいか（例：loss が NaN にならないか、OOM が出ないか）
   - 予想される GPU メモリ使用量や時間感
   - 実行上の注意点（別環境での実行、screen/tmux 利用など）

   をコメントとして追記してください。

3. これらを  
   `LLM_ファインチューニング/StepE_タスク2_結果.md`  
   にまとめてください。

> ※ Codex 側では、トライアル学習コマンドは **提案のみ** に留めてください。  
>  実際の実行はユーザーが行います（もしユーザーが許可した場合を除く）。

---

## タスクE-3：Fine-tuned モデルの読み込み＆評価方法の設計

### 目的

学習済みLoRAモデルを statutes-rags の評価パイプラインから呼び出す方法を設計します。

### 前提

- 現状の `evaluate_multiple_choice.py` は Ollama（`qwen3:8b`）を使っています。
- Fine-tuning 用スクリプトは HF モデル（例：`Qwen/Qwen1.5-7B-Chat`）+ LoRA で動かす想定です。
- つまり、**Baseモデル（Ollama）と FTモデル（HF+LoRA）のバックエンドが異なる**可能性があります。

### やってほしいこと

1. 次の2案を比較しつつ、**どちらで進めるか提案してください**。

   - 案1：statutes-rags に「HFローカルLLM」を追加
     - `rag_config` に `llm_backend = "ollama" | "hf"` のようなフィールドを追加し、
     - `llm_backend="hf"` の場合は、
       - HF Transformers + LoRA モデルをロードして `RAGPipeline` から利用する。
     - evaluate_multiple_choice 側では `--llm-backend hf` などで切り替え。
   - 案2：LoRA学習した重みを Ollama 用フォーマットに変換し、`qwen3-law-ft` のような新モデルとして Ollama 側に登録する
     - 簡単ではないため、現時点では「将来の拡張案」として扱ってよい。

2. 案1 を採用する場合を前提に、次の設計を Markdown でまとめてください。

   - どのファイルを編集するか（例：`rag_config.py`, `rag_pipeline.py`, `app/llm/` 以下など）
   - HF モデルの読み込み処理をどこに書くか（新規モジュール案など）
   - LoRA 重みのパスをどこで指定するか（`rag_config` のフィールドか環境変数か）

3. 上記の設計案を  
   `LLM_ファインチューニング/StepE_タスク3_設計.md`  
   として保存してください。

> ※ このタスクではまだコードを書かないでください。  
>  どのファイルをどう変えるか、設計を言語化するのみです。

---

## タスクE-4：Fine-tuned モデル評価用のコマンド案＆結果ログテンプレ

### 目的

Base vs FT モデルを比較するための **評価コマンド案** と、  
それを記録する **評価ログテンプレート** を用意します。

### やってほしいこと

1. タスクE-3 で決めた設計が「HFローカルLLM追加」だったと仮定して、  
   evaluate_multiple_choice.py を使って **140問評価**するためのコマンド案を作ってください。

   - Base HFモデル用（LoRAなし）
   - FT HFモデル用（LoRA適用）

   それぞれについて、

   - RAGあり + direct（CoTなし）
   - RAGあり + CoT

   少なくとも 2〜4 パターンをコマンドとして提案してください。

2. このコマンド案を  
   `LLM_ファインチューニング/StepE_評価コマンド案.md`  
   にまとめてください。

3. 別ファイルとして、  
   `LLM_ファインチューニング/StepE_評価ログテンプレート.md`  
   を作成し、各評価実験のログを書き込むためのテンプレートを作ってください。

   - 含めたい項目例：
     - 実験ID
     - 使用モデル（Base / FT / バックエンド種別）
     - retriever設定（RAG有無, top_k, reranker）
     - プロンプト設定（direct / CoT / few-shot）
     - 正答率（全体）
     - 比較対象（Baseとの差分など）
     - 実行コマンド
     - 所感・今後の改善ポイント

> ※ このタスクでも、コマンドは提案のみで、実際の実行はユーザーに委ねてください。

---

## タスクE-5：StepE全体のまとめ

### 目的

学習〜評価フローが複雑になってきたため、StepEの成果を1枚にまとめておきます。

### やってほしいこと

1. StepE-1〜E-4 で作成・変更したファイルを一覧にし、

   - スクリプト側の変更点（`train_qwen_law_ft.py`、評価パイプラインの設計など）
   - 学習コマンド案
   - 評価コマンド案
   - ログテンプレート

   を簡潔にまとめてください。

2. 「今後、人間がやるべきこと」を箇条書きで整理してください。

   - 例：
     - トライアル学習コマンドをどの環境でいつ実行するか
     - 本番学習をどのパターンでどれくらい回すか
     - Base vs FT の評価をどの順で行うか

3. 上記を  
   `LLM_ファインチューニング/StepE_まとめ.md`  
   として保存してください。

---

以上のタスクを、この順番で実行してください。  
各タスクの完了時には、対応する `StepE_タスクX_結果.md` などに  
「何を変更・設計・提案したか」を必ず記録してください。
