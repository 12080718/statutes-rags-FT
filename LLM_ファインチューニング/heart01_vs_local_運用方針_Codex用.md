# heart01（A100）とローカル（3080+Codex）の役割分担方針（Codex用）

このドキュメントは、**VS Code + Codex が使えるローカル環境（RTX3080）** と  
**鹿児島大学 GPU サーバー heart01（A100 20GB/MIG）** の役割分担を整理したものです。

ポイントは次の 2 つです。

1. **「コードを書く場所」と「重い実験を回す場所」を分離する**
2. **heart01 では HF バックエンドだけ使い、Ollama は使わない（or 最低限にとどめる）**

以降、Codex で補助する際は、この方針を前提として指示を解釈してください。

---

## 1. 背景と制約

- 研究タスク：  
  - 法令 4 択問題の精度向上  
  - LLM: **Qwen3:8B 系** を共通条件とし、fine-tuning（QLoRA）で精度向上を狙う
- 利用 GPU：
  - ローカル：RTX3080（10GB）
  - 大規模：heart01 上の A100 80GB（ただし MIG により **20GB スライス**を使用）
- 制約：
  - heart01 では **Codex が使えない**
  - heart01 では **curl が使えない（Ollama 公式インストーラがそのまま動かない）**
  - そのため、heart01 上で Ollama サーバを立ててガンガン叩く運用は現実的ではない

---

## 2. 基本方針（とても重要）

今後の開発・実験では、**次のように役割を分けます**。

### ローカル（RTX3080 + Codex）

- 役割：
  - **コード開発・設計・リファクタリングの主戦場**
  - Codex を最大限使い、Python スクリプトや設定ファイルの編集を行う
- 主に行うこと：
  - `train_qwen_law_ft.py` / `evaluate_multiple_choice.py` / `app/llm/hf_llm.py` などの実装
  - 小さい HF モデル（例：1.5B〜4B）を使った **パイプラインのデバッグ**
  - 必要に応じて、Ollama + Qwen3:8B を使ったベースライン評価（ローカルだけで完結）
  - 完成したコードを GitLab/GitHub 等に push する

### heart01（A100 20GB / MIG）

- 役割：
  - **本番規模の学習・評価を実行する場所**
  - ここでは基本的に「コードはほぼ編集しない」「Codex は使わない」
- 主に行うこと：
  - リポジトリの clone / pull
  - Python 仮想環境構築・依存インストール
  - **Hugging Face バックエンド（HF）での Qwen3:8B 4bit QLoRA 学習**
  - HF バックエンドでの 140問フル評価（direct / CoT / RAG 有無など）

---

## 3. heart01 上での禁止事項・注意事項

今後、Codex は **heart01 で実行するコマンドやスクリプト例を提案することはある** ものの、  
**heart01 上に Codex を「インストールして使おう」とはしないでください**。

また、以下の方針を守る前提でスクリプト案を出してください。

### 3.1 Ollama は heart01 では使わない

- heart01 では以下を **しない**：
  - `curl https://... | sh` 形式の Ollama インストール
  - Ollama サーバーを立ち上げて、`http://localhost:11434` を叩く評価
- 理由：
  - ポリシー的に curl 禁止・長時間常駐プロセスに制約がある
  - 本番実験は **HF モデル＋QLoRA だけで十分** だから

→ Codex が heart01 用のスクリプトを設計するときは、  
**必ず HF バックエンド（`transformers` / `peft` / `bitsandbytes`）前提で書いてください。**

### 3.2 curl ではなく wget / pip を前提にする

- heart01 では `curl` が使えないため、
  - インストール例を出す場合は `wget` + `sh`  
    あるいは Python / pip のみで完結する形で提示してください。
- 良い例：
  - `pip install -r requirements.txt`
  - `wget https://.../some_script.sh && bash some_script.sh`
- NG 例：
  - `curl https://... | sh` だけを提示する

---

## 4. 実験設計上の整理（Codex が意識すべき前提）

Codex が実験コマンドやログテンプレートを生成するとき、  
以下の前提を常に意識してください。

### 4.1 LLM モデルの整理

- チーム共通条件としての LLM：
  - **Qwen3:8B 系**（ベースライン＆本番評価はこれが軸）
- 実験のバリエーション：
  - ローカル（3080）：
    - 小さい HF モデル（1.5B〜4B）での QLoRA 学習（パイプライン検証用）
    - Ollama Qwen3:8B を使ったベースライン（参考）
  - heart01（A100）：
    - HF Qwen3:8B + 4bit QLoRA での学習（本番）
    - HF Qwen3:8B での評価（ベースライン＆FT後）

Codex は、**どの環境でどのモデルを使うべきか**を指示文中で明確に区別してください。

### 4.2 スクリプト利用の基本線

- 学習：
  - `scripts/train_qwen_law_ft.py`
    - 主に heart01 で **Qwen3:8B + QLoRA 4bit** を学習する
    - ローカルでは 1.8B などの小モデルでテストする
- 評価：
  - `scripts/evaluate_multiple_choice.py`
    - ローカル：Ollama / HF 小モデルで小規模テスト
    - heart01：HF Qwen3:8B + LoRA で 140問フル評価

Codex は、heart01 用のコマンド例を出すときは、  
**必ず `--llm-backend hf` / `--hf-model-name` / `--hf-lora-path` を使った形で書いてください。**

---

## 5. Codex にお願いしたいこと（今後の StepI 以降）

この方針を踏まえて、今後 Codex には主に以下をお願いしていきます。

1. **ローカル（3080 + Codex）側でのコード整備**
   - Python スクリプト・設定ファイルの修正
   - ローカルでの小モデルデバッグ（StepH のようなタスク）

2. **heart01（A100）側で実行するコマンドや手順書（md）の作成**
   - 例：
     - `StepI_A100_Qwen3_8B_本番学習指示.md`
     - `StepJ_A100_Qwen3_8B_評価指示.md`
   - 中身は「heart01 でユーザが手で実行するコマンド・確認手順」の形で書く

3. **両環境で使うログテンプレート・結果整理用 md の生成**
   - ベースライン結果テンプレ
   - 学習ログテンプレ
   - 評価ログテンプレ
   など、既にあるものを拡張・再利用する形で整備する

---

## 6. まとめ

- heart01 上では Codex や Ollama を直接動かさず、**HF バックエンドのみで Qwen3:8B の学習＆評価を行う。**
- Codex は **ローカル環境（3080）での開発支援** と  
  **heart01 でユーザが実行するための指示 md / コマンド例の生成** に集中する。

以降、この方針を前提に StepI 以降の指示文やスクリプト案を作成してください。