## 本番 140問ベースライン実行コマンド案（実行しないでください）

前提: lawqa_jp selection（140問）、インデックスは構築済み、LLMは Ollama の `qwen3:8b`。Rerankerなし/ありの2パターンを提示。時間は環境次第（CPUのみだと数十分〜、GPU/高速LLMなら短縮）。

### パターン1: RAGあり・CoTなし（directベースライン、top_k=3）
```
python scripts/evaluate_multiple_choice.py \
  --data datasets/lawqa_jp/data/selection.json \
  --output results/evaluations/baseline_rag_qwen3_full.json \
  --top-k 3 \
  --llm-model qwen3:8b \
  --ensemble 1
```
- RAG有効（デフォルト retriever_type は config 依存、top_k=3）
- Few-shot有効（デフォルト）、CoTなし
- 140問全件評価

### パターン2: RAGあり・CoTあり（step-by-step、top_k=3）
```
python scripts/evaluate_multiple_choice.py \
  --data datasets/lawqa_jp/data/selection.json \
  --output results/evaluations/baseline_rag_qwen3_full_cot.json \
  --top-k 3 \
  --llm-model qwen3:8b \
  --ensemble 1 \
  --use-cot
```
- RAG有効、CoTプロンプトで推論
- Few-shot有効（デフォルト）
- 140問全件評価

### オプション（任意）
- Rerankerを使う場合（top_k自動調整あり）:
```
  --use-reranker --reranker-model cross-encoder/ms-marco-MiniLM-L-12-v2 --rerank-top-n 3
```
- Few-shotを無効にする場合: `--no-few-shot`
- インデックス未構築の場合は事前に `make index` を実施

### 実行時間の目安（コメント）
- CPUのみ・Ollamaローカルの場合、140問で数十分かかる可能性あり。
- GPU有りや高速モデル設定で短縮可。タイムアウトはデフォルト120秒。
